{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiY-OBq3P99p"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading CIFAR-10 dataset\n",
        "cifar_data = tf.keras.datasets.cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar_data.load_data()"
      ],
      "metadata": {
        "id": "brCwqzdGQD07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert RGB images to grayscale\n",
        "def oneDtotwoD(X):\n",
        "  X1 = []\n",
        "  for i in range(X.shape[0]):\n",
        "    img1 = X[i,:,:,:]\n",
        "    img_gray = 0.2989 * img1[:,:,0] + 0.5870 * img1[:,:,1] + 0.1140 * img1[:,:,2]\n",
        "    X1.append(img_gray)\n",
        "    print(len(X1))\n",
        "  return X1"
      ],
      "metadata": {
        "id": "IGW3a-6oQGek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting training images to grayscale\n",
        "X_train1 = oneDtotwoD(X_train)\n",
        "# Converting list to numpy array\n",
        "X_train1 = np.array(X_train1)\n",
        "print(X_train1.shape)\n",
        "# Reshaping and normalizing the original training and test data (color images)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2] * X_train.shape[3]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2] * X_test.shape[3]))\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# Checking the shape of reshaped and normalized data\n",
        "print(X_train.shape, X_test.shape)\n",
        "# Converting test images to grayscale\n",
        "X_test1 = oneDtotwoD(X_test)\n",
        "X_test1 = np.array(X_test1)\n",
        "# Reshaping and normalizing the grayscale training and test data\n",
        "X_train2 = np.reshape(X_train1, (X_train1.shape[0], X_train1.shape[1] * X_train1.shape[2]))\n",
        "X_test2 = np.reshape(X_test1, (X_test1.shape[0], X_test1.shape[1] * X_test1.shape[2]))\n",
        "X_train2 = X_train2 / 255\n",
        "X_test2 = X_test2 / 255\n",
        "X_train2 = X_train2.astype('float32')\n",
        "X_test2 = X_test2.astype('float32')"
      ],
      "metadata": {
        "id": "GpG3EJ8TQHA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Autoencoder model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=512, activation='sigmoid', input_shape=(784,))) # Hidden Dimension or Latent Representation\n",
        "model.add(tf.keras.layers.Dense(units=784, activation='sigmoid'))  # Output layer with same number of units as in the input layer\n",
        "# Compiling the model\n",
        "model.compile(loss='MeanSquaredError', optimizer='adam', metrics=['MeanSquaredError'])\n",
        "# Displaying the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "omn5jLp0QsMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "epochs = 100\n",
        "history = model.fit(X_train2, X_train2, epochs=epochs, validation_data=(X_test2, X_test2), batch_size=128, verbose=2)"
      ],
      "metadata": {
        "id": "N-yTzhgbRGE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting loss for 100 epochs using Adam optimizer\n",
        "num_epochs = np.arange(0, 100)\n",
        "plt.figure()\n",
        "plt.plot(num_epochs, history.history['loss'], label='Training Loss')\n",
        "plt.plot(num_epochs, history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A48V0cmPRH21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on test data (grayscale images)\n",
        "X_pred = model.predict(X_test2[:10, :])\n",
        "X_pred = np.reshape(X_pred, (X_pred.shape[0], 32, 32))\n",
        "plt.figure()\n",
        "# Displaying the first 9 predicted images\n",
        "for i in range(1, 10):\n",
        "  plt.subplot(5, 2, i)\n",
        "  plt.imshow(X_pred[i-1, :, :])\n",
        "# Reshaping test data for visualization\n",
        "X_test1 = X_test[:10, :]\n",
        "X_test1 = np.reshape(X_pred, (X_pred.shape[0], 32, 32))\n",
        "X_test1 = X_test1 * 256  # Scaling the test images for better visualization\n",
        "plt.figure()\n",
        "# Displaying the first 9 test images\n",
        "for i in range(1, 10):\n",
        "  plt.subplot(5, 2, i)\n",
        "  plt.imshow(X_test1[i-1, :, :])"
      ],
      "metadata": {
        "id": "Ee2yIOkHRbvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}